{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (0.16.1)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.2-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchaudio-2.1.1-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Downloading torchaudio-2.1.1-cp39-cp39-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 2.3/2.3 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (d:\\anaconda\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/ 依官網所示\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertTokenizerFast,AutoModelForCausalLM\n",
    "#----->沒使用pipeline,自行設定套件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. gpt2-small: The smallest version, with 117 million parameters.\n",
    "2. gpt2-medium: The medium-sized version, with 345 million parameters.\n",
    "3. gpt2-large: The large version, with 774 million parameters.\n",
    "4. gpt2-xl: The largest version, with 1.5 billion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a49642afe034ece977fe87209b42746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5b9fdbb4ae4c6e88f09a78eb41fb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26135749467e4549953ad80c8b59d2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a346d0bdad9d478c8f5a7d861f7a5b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf586c1c90846aaa8529bfd59355b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b258f62d364453dbd1afe66935b1440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 15:25:15.515694: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Input': 'I have a pen, I have an iphone, I have a laptop, I'}\n"
     ]
    }
   ],
   "source": [
    "#產生句子\n",
    "input_txt = \"I have a pen, I have an \"\n",
    "# tokenizer()會返回單詞編號:input_ids/類型:token_type_ids/是否mask:attention_mask\n",
    "#'tf'：傳回 TensorFlow tf.constant 物件。'pt'：傳回 PyTorch torch.Tensor 物件。'np'：傳回 Numpy np.ndarray 物件。\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"]#.to(device)\n",
    "iterations = []\n",
    "n_steps = 10\n",
    "choices_per_step = 3\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids)\n",
    "        # 選最後一個 token 然後過 softmax 後選出機率最大\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "print(iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a pen, I have an iphone, I have a laptop. Thus, I have a lot of things that I can do. I can do a lot of things. I can do a lot of things. I can do a lot of things. I can do a lot of things. I can\n"
     ]
    }
   ],
   "source": [
    "max_length = 64\n",
    "\n",
    "input_txt = \"\"\"I have a pen, I have an iphone, I have a laptop. Thus,\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=max_length)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese')\n",
    "#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=256\n",
    "input_txt = \"\"\"\n",
    "隨著貸款日益枯竭，Alistair Darling 被迫考慮對銀行進行第二次救助。 \\\n",
    "財政大臣將在幾週內決定是否向經濟中再注入數十億美元，因為有證據表明\\\n",
    "去年 370 億的部分國有化未能保持信貸流動，\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 隨 著 貸 款 日 益 枯 竭 ， [UNK] [UNK] 被 迫 考 慮 對 銀 行 進 行 第 二 次 救 助 。 財 政 大 臣 將 在 幾 週 內 決 定 是 否 向 經 濟 中 再 注 入 數 十 億 美 元 ， 因 為 有 證 據 表 明 去 年 370 億 的 部 分 國 有 化 未 能 保 持 信 貸 流 動 ， [SEP] 得 它 自 身 對 房 地 產 的 依 賴 程 度 增 加 。 [UNK] [UNK] [UNK] （ ） 是 香 港 一 個 以 [UNK] [UNK] 為 發 售 的 免 費 電 訊 軟 體 平 臺 ， 由 香 港 無 綫 電 視 聯 播 有 限 公 司 製 作 ， 由 陳 奕 迅 、 譚 嘉 銘 、 陳 浩 然 、 周 潤 發 、 陳 麗 芬 及 [UNK] [UNK] 發 起 。 目 前 為 一 個 平 臺 ， 於 2017 年 6 月 31 日 起 每 平 方 英 文 電 腦 下 載 （ [UNK] + 1 ） ， 但 由 於 當 時 是 有 線 電 視 的 開 放 ， 此 舉 造 成 消 費 者 的 困 擾 ， 令 不 少 網 民 不 能 安 心 到 此 ， 為 了 滿 足 電 訊 廣 播 服 務 商 的 需 求 ， 香 港 電 話 臺 每 天 均 進\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"]#.to(device)\n",
    "output = model.generate(input_ids, max_length=max_length, num_beams=1,  do_sample=True, top_k=50)\n",
    "print(tokenizer.decode(output[0]))\n",
    "#print(tokenizer.decode(output[0], skip_special_tokens=True))# 去掉 Special Token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 隨 著 貸 款 日 益 枯 竭 ， [UNK] [UNK] 被 迫 考 慮 對 銀 行 進 行 第 二 次 救 助 。 財 政 大 臣 將 在 幾 週 內 決 定 是 否 向 經 濟 中 再 注 入 數 十 億 美 元 ， 因 為 有 證 據 表 明 去 年 370 億 的 部 分 國 有 化 未 能 保 持 信 貸 流 動 ， [SEP] 得 它 自 身 對 房 地 產 的 依 賴 程 度 增 加 。 [UNK] [UNK] [UNK] （ ） 是 香 港 一 個 以 [UNK] [UNK] 為 發 售 的 免 費 電 訊 軟 體 平 臺 ， 由 香 港 無 綫 電 視 聯 播 有 限 公 司 製 作 ， 由 陳 奕 迅 、 譚 嘉 銘 、 陳 浩 然 、 周 潤 發 、 陳 麗 芬 及 [UNK] [UNK] 發 起 。 目 前 為 一 個 平 臺 ， 於 2017 年 6 月 31 日 起 每 平 方 英 文 電 腦 下 載 （ [UNK] + 1 ） ， 但 由 於 當 時 是 有 線 電 視 的 開 放 ， 此 舉 造 成 消 費 者 的 困 擾 ， 令 不 少 網 民 不 能 安 心 到 此 ， 為 了 滿 足 電 訊 廣 播 服 務 商 的 需 求 ， 香 港 電 話 臺 每 天 均 進'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = tokenizer.decode(output[0])\n",
    "doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
